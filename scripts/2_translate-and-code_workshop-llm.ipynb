{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPZNPoIAdSG+oHp7uP5W6aD"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["LLM Workshop Use Case 2: Translation and coding of posts.\n","\n","Author: Simon van Baal\n","\n","Date: 20240405"],"metadata":{"id":"YnitZ49U2kYN"}},{"cell_type":"markdown","source":["# Setup"],"metadata":{"id":"GqqEXI6j0MwQ"}},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CDeP0kBXc2P7","executionInfo":{"status":"ok","timestamp":1717593555213,"user_tz":-120,"elapsed":27797,"user":{"displayName":"Simon van Baal","userId":"09750679850689710630"}},"outputId":"0eace4df-743a-4daa-daf4-7f99d06d1407"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting openai\n","  Downloading openai-1.31.0-py3-none-any.whl (324 kB)\n","\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m324.1/324.1 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n","Collecting httpx<1,>=0.23.0 (from openai)\n","  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n","\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.7.2)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n","Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n","Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.0)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n","Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n","  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n","\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n","  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n","\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.18.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.18.3)\n","Installing collected packages: h11, httpcore, httpx, openai\n","Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 openai-1.31.0\n"]}],"source":["# Install custom package\n","!pip install openai\n","\n","# Import packages\n","from google.colab import drive\n","import pandas as pd\n","import os\n","from openai import OpenAI\n","\n","from transformers import pipeline"]},{"cell_type":"code","source":["# Mount google drive to enable access to data files\n","\n","drive.mount('/content/drive/')\n","# You will be asked to give permission here."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qp4f4dZ2dXO7","executionInfo":{"status":"ok","timestamp":1717593577849,"user_tz":-120,"elapsed":22640,"user":{"displayName":"Simon van Baal","userId":"09750679850689710630"}},"outputId":"fba6019f-f19e-41c9-8e1f-bd8d6084a616"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}]},{"cell_type":"markdown","source":["## Working Directory\n","\n","Now here is a tricky bit. The command below will set the working directory. The bit up to and including \"My Drive\" will be the same (even if your google drive is in a different language). If you have this at the top of your Drive hierarchy, you may simply delete \"projects/\" below, and it should run.\n","\n","In general, it is important you check the output carefully, since even though the chunk below will tell you if it could not find the directory, it will still run!"],"metadata":{"id":"-4t31ki-zRzA"}},{"cell_type":"code","source":["# You will need to change this line below to suit your directory, unfortunately.\n","%cd /content/drive/My Drive/projects/workshop-llm_uni-wien-main/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t_ssFrAxdlb7","executionInfo":{"status":"ok","timestamp":1717593518033,"user_tz":-120,"elapsed":9,"user":{"displayName":"Simon van Baal","userId":"09750679850689710630"}},"outputId":"3219a718-7bbd-494d-d480-8da5b69b4d33"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["[Errno 2] No such file or directory: '/content/drive/My Drive/projects/workshop-llm_uni-wien-main/'\n","/content\n"]}]},{"cell_type":"code","source":["#Use this to see if you are in the right folder\n","os.listdir()\n","\n","#Load in some data.\n","df = pd.read_csv(\"data/data_workshop-llm.csv\")"],"metadata":{"id":"Y9jZnBz6dKnj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Find the right model for your translation task on the Hugging Face model hub.\n","# https://huggingface.co/models - simply copy the title of the model and paste.\n","\n","translate_de = pipeline(\"translation\",\n","                        model = \"Helsinki-NLP/opus-mt-de-en\",\n","                        max_length = 800)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0rbhknoMe3Ha","executionInfo":{"status":"ok","timestamp":1712315452776,"user_tz":-120,"elapsed":5646,"user":{"displayName":"Simon van Baal","userId":"09750679850689710630"}},"outputId":"586bc1f4-5784-4133-ab29-19870f8b5fc3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/models/marian/tokenization_marian.py:197: UserWarning: Recommended: pip install sacremoses.\n","  warnings.warn(\"Recommended: pip install sacremoses.\")\n"]}]},{"cell_type":"code","source":["translate_de(\"Hallo, wie geht's?\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SXnUL657qez7","executionInfo":{"status":"ok","timestamp":1712315453704,"user_tz":-120,"elapsed":933,"user":{"displayName":"Simon van Baal","userId":"09750679850689710630"}},"outputId":"2eff0c2f-cb9a-475c-b776-c48d1fa615ff"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'translation_text': 'Hello, how are you?'}]"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["# Let us wrap a function, so it is easier to repeat the same operation.\n","def translate_to_english(text, language=\"German\"):\n","  # The default is\n","  if language == \"German\":\n","    try:\n","      translation = translate_de(text)\n","    except:\n","      print(\"something went wrong with the text input.\")\n","  else:\n","    print(\"no other translators have been set yet.\")\n","  return translation\n","\n"],"metadata":{"id":"UUtb_IgygRO7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Note that Python starts indexing from zero, not from one, so the line below\n","# selects the second line of text.\n","df_translation = pd.DataFrame()\n","\n","for row in range(0,10):\n","  # Grab post from df.\n","  post = df.iloc[row]\n","\n","  # Translate it.\n","  english_text = translate_to_english(str(post[\"content\"]))\n","  print(english_text)\n","\n","  # Create an output list\n","  output = [[\"channel\", \"user_id\", \"content\", \"timestamp\", \"views\", \"translation\"]]\n","  output.append(list(post[[\"channel\", \"user_id\", \"content\", \"timestamp\", \"views\"]]) + [\n","                                english_text[0]])\n","\n","  # Convert it to a data frame so we can save to csv.\n","  df_out=pd.DataFrame(output[1:],\n","                        columns=output[0])\n","  df_out.to_csv(\"output/post\"+str(row)+\"_workshop.csv\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5LMigX7Iiav2","executionInfo":{"status":"ok","timestamp":1712315532933,"user_tz":-120,"elapsed":79232,"user":{"displayName":"Simon van Baal","userId":"09750679850689710630"}},"outputId":"9cc2e60e-c0e2-4d42-c795-2a2187ca8c3f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[{'translation_text': 'Livechat Monday 31 October 2022 8 pm with **Hannes Brejcha**üòâ ÁöÑ[ Hier](https://t.me/Fairdenkenoriginal)'}]\n","[{'translation_text': '\"The energy crisis hits private households with full force: Gas and electricity prices have now reached a record level. From 2023 on, consumers even have to adjust to a tripling of reductions. In addition, the heating season soon begins and many people are faced with the question: How can I save energy? Business Insider has talked to ten young people about how they are in crisis and how they save energy. In the talks, we have learned that despair seems to be still limited ‚Äì rather, they are trying to find creative solutions.\" https://www.businessinsider.de/politics/germanland/I-will-me-now-one-flowerpot-oven-building-ten-young-people-give-tips-to-energysaving/'}]\n","[{'translation_text': '*At the end of last week, the fourth Corona vaccine was approved in the EU with the vector vaccine of Johnson & Johnson. The approval in the USA was already given a few weeks before ‚Äì and called on the Catholic Church to do so. The archdiocese of New Orleans declared appropriate vaccinations \"morally compromised\" referring to the \"extensive use of cell lines derived from abortions.\" https://www.compact-online.de/bischoefe-warnen-vaccine-with-cell-lines-the-expulsion-trimmies/'}]\n","[{'translation_text': 'At last, the whole Covid-19 story becomes the file for the prosecutor. Responsiblely, ¬ªLawyers for Enlightenment¬´ have gone over to fighting with more modern, as it were, harder bandages. Together with the plagued people, they now grab the famous bull directly by the horns. Previously, they exposed the PCR test as the only major fundamental irrtum (= fraud). And this together with truly ¬ªPharma Lobby-independent coryphaes¬´ from all the necessary backgrounds: high-grade virologists, epidemiologists, microbiologists, doctors, etc.+ ‚úì www.kla.tv/18358 --------------------------------------------------------------------------------- https://t.me/Kampf_fuer_our_Future/1058 -------------------------------------------------'}]\n","[{'translation_text': 'https://immunzen-nein-danke.de/Spanish flu Thank you very much for sending us this message üòâ @BlutgruppeQ'}]\n","[{'translation_text': 'https://youtu.be/SQYttHbELbE'}]\n","[{'translation_text': 'Mike Lindell is back on Twitter at https://twitter.com/realMikeLindell/status/1603846182455083009?t=TB4oUdbWHj6hzRHB9cB1Yg&s=19 https://t.me/danijelsheran'}]\n","[{'translation_text': 'Despite increasing incidences, towards the end of the summer, the Danes will be able to regain their freedoms from October onwards. Meanwhile, pressure on unvaccinated citizens will be increased in Germany. https://www.epochtimes.de/politik/ausland/daenemark-buerger-erhalten-ab-oktober- ihre-freiheiten-zurueck-a3572658.html?telegram=1 https://t.me/epochtimesde'}]\n","[{'translation_text': 'https://youtu.be/lTntb1rXZNg'}]\n","[{'translation_text': 'The U.S. rapper Coolio, known for his hit \"Gangsta\" Paradise, continued the mysterious trend of sudden demise for no apparent reason. It happened at a friend\\'s home in L.A. - investigators could not find any drugs in the house. Five days ago Coolio shared a video of his last concert on his Instagram account, where he seemed absolutely fit. This post should also be his last - now his fans are condoning and grieving. The German FB group is still online: [Pl√∂tlich und unexpected](https://t.me/rabbitresearch/9032), which deals with this very phenomenon of sudden extinction, the Plattfoprm.'}]\n"]}]},{"cell_type":"markdown","source":["# Fancier Models\n","\n","Now we proceed into the domain of using more cutting-edge models.\n","\n","The first problem is that most of our machines don't have enough working memory to hold these models - many have more than 7 billion parameters. And we would need to pay for Google Colab Pro to use more RAM.\n","\n","The second problem is that many top-of-the-line models are not open source, so we need to pay a (usually small) fee to use them.\n","\n","Now let's try and use OpenAI their models. You first need to sign up, and then -- hopefully -- you get $5 free. If you wish to do so, head to platform.openai.com. To generate an API key, for authentication, go to the menu on the left to API Keys and press \"create secret key\"."],"metadata":{"id":"FPZW85QEs0Ak"}},{"cell_type":"code","source":["# Set up connection to OpenAI; generate an API key, and\n","# paste it in between the quotes below.\n","client = OpenAI(\n","    api_key='',\n","    timeout=30\n",")\n","\n","# =========================== Define primary coding function\n","def chat(system_msg,\n","         user_assistant,\n","         model,\n","         temperature,\n","         top_p,\n","         json):\n","  assert isinstance(system_msg, str), \"`system_msg` should be a string\"\n","  assert isinstance(user_assistant, list), \"`user_assistant` should be a list\"\n","  assert isinstance(json, bool), \"`json` should be one of True/False\"\n","\n","  # Define inputs to LLM, first we have \"system message\" to give it guidance.\n","  system_msg = [{\"role\": \"system\",\n","                 \"content\": system_msg}]\n","\n","  # We now feed it the context of the conversation that occurred before.\n","  # if role = assistant, it was the LLM speaking, if role = user, it was us.\n","  user_assistant_msgs = [\n","        {\"role\": \"assistant\",\n","         \"content\": user_assistant[i]} if i % 2 else {\"role\": \"user\",\n","                                                      \"content\": user_assistant[i]}\n","        for i in range(len(user_assistant))\n","        ]\n","  msgs = system_msg + user_assistant_msgs\n","\n","  # If we specify no json output, we will get a regular text response.\n","  if not json:\n","    response = client.chat.completions.create(\n","        model=model,\n","        messages=msgs,\n","        temperature=temperature,\n","        seed=1,\n","        top_p=top_p\n","    )\n","  elif json and not any(new_model in model for new_model in [\"gpt-4\", \"gpt-3.5-turbo\"]):\n","    raise ValueError(\"You have selected JSON output. This model cannot handle that.\")\n","\n","  # Otherwise proceed and produce JSON output.\n","  response = client.chat.completions.create(model=model,\n","                                              messages=msgs,\n","                                              temperature=temperature,\n","                                              top_p=top_p,\n","                                              seed=1,\n","                                              response_format={\"type\": \"json_object\"})\n","\n","  return response\n"],"metadata":{"id":"TZr4fQ0gjLLy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["codable_post = pd.read_csv(\"output/post2_workshop.csv\", index_col=False)\n","\n","system = (\"You are an assistant helping me code posts from Telegram about war\"\n","          \" and vaccination.\"\n","          \" Please tell me whether the post I enter is about war, vaccination,\"\n","          \" or neither.\"\n","          \" Answer in JSON format: {'cat': category of post [war/vax/none],\"\n","          \" 'stance': whether it is pro war/vaccination, or anti [pro/anti/NA].}\")\n","\n","# Select the post. I am selecting the original German here, which it can handle.\n","prompt = [str(codable_post.iloc[0, 3])]\n","\n","# Q: How would you select the translated column?\n","\n","model_output = chat(system_msg = system,\n","                user_assistant = prompt,\n","                model = \"gpt-3.5-turbo\",\n","                temperature = 1,\n","                top_p = .9,\n","                json = True)\n","\n"],"metadata":{"id":"qXaCKofbsyKJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# We can see it adequately shows us what is happening in this post.\n","print(model_output.choices[0].message.content)\n","\n","\n","# The way this is set up, if we save the user prompt and the LLM message in a list,\n","# and make a new request afterwards, it will be automatically passed to the\n","# model in the correct order."],"metadata":{"id":"AqbpQgqy5cM9"},"execution_count":null,"outputs":[]}]}